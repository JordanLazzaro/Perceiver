{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdVA2H0mbl-Z",
        "outputId": "0b380308-d023-4058-fe7a-0a5a2a01e890"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Global seed set to 42\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install -q wandb pytorch_lightning\n",
        "\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchmetrics import Accuracy\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning.tuner import Tuner\n",
        "\n",
        "import wandb\n",
        "\n",
        "pl.seed_everything(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XsX4ZVTDq1sd"
      },
      "outputs": [],
      "source": [
        "class LatentEmbeddings(nn.Module):\n",
        "    def __init__(self, latent_seq_len, latent_channels):\n",
        "        ''' stole this bad boy from HF transformers perceiver impl. '''\n",
        "        super().__init__()\n",
        "        self.latents = nn.Parameter(torch.randn(latent_seq_len, latent_channels))\n",
        "\n",
        "    def forward(self, batch_size):\n",
        "        # we want to have the same latents across batch dimension\n",
        "        return self.latents.expand(batch_size, -1, -1)\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        latent_channels,\n",
        "        in_channels,\n",
        "        qk_channels=None,\n",
        "        v_channels=None,\n",
        "        out_channels=None,\n",
        "        nxheads=1,\n",
        "        dropout=0.0\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # we want to default to latent channels for q/k/v\n",
        "        if qk_channels is None:\n",
        "            qk_channels = latent_channels\n",
        "        if v_channels is None:\n",
        "            v_channels = qk_channels\n",
        "        if out_channels is None:\n",
        "            # not sure why deepmind code defaults to v_channels since we want\n",
        "            # the final channel number to match latent channels regardless\n",
        "            out_channels = latent_channels\n",
        "\n",
        "        assert qk_channels % nxheads == 0\n",
        "        assert v_channels % nxheads == 0\n",
        "\n",
        "        self.ln_1atent = nn.LayerNorm(latent_channels)\n",
        "        self.ln_input = nn.LayerNorm(in_channels)\n",
        "\n",
        "        self.W_Q = nn.Linear(latent_channels, qk_channels, bias=False)\n",
        "        self.W_K = nn.Linear(in_channels, qk_channels, bias=False)\n",
        "        \n",
        "        self.W_V = nn.Linear(in_channels, v_channels, bias=False)\n",
        "        self.W_O = nn.Linear(v_channels, out_channels, bias=False)\n",
        "\n",
        "        self.v_channels = v_channels\n",
        "\n",
        "        self.qk_head_dim = qk_channels // nxheads \n",
        "        self.v_head_dim = v_channels // nxheads\n",
        "        \n",
        "        self.nxheads = nxheads\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, latent_q, input_kv):\n",
        "        batch_size, input_seq_len, in_channels = input_kv.size()\n",
        "        _, latent_seq_len, latent_channels = latent_q.size()\n",
        "\n",
        "        # (batch_size, (latent/input)_seq_len, latent_channels) -> (batch_size, nheads, (latent/input)_seq_len, (qk/v)_head_dim)\n",
        "        Q = self.W_Q(latent_q).reshape(batch_size, latent_seq_len, self.nxheads, self.qk_head_dim).transpose(1, 2)\n",
        "        K = self.W_K(input_kv).reshape(batch_size, input_seq_len, self.nxheads, self.qk_head_dim).transpose(1, 2)\n",
        "        V = self.W_V(input_kv).reshape(batch_size, input_seq_len, self.nxheads, self.v_head_dim).transpose(1, 2)\n",
        "\n",
        "        # (batch_size, nheads, latent_seq_len, input_seq_len)\n",
        "        attn = (Q @ K.transpose(-2, -1)) / (1.0 * math.sqrt(self.qk_head_dim))\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        \n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        # (batch_size, nheads, latent_seq_len, v_head_dim)\n",
        "        out = attn @ V\n",
        "        # (batch_size, latent_seq_len, v_channels)\n",
        "        out = out.transpose(1, 2).reshape(batch_size, latent_seq_len, self.v_channels)\n",
        "\n",
        "        # (batch_size, latent_seq_len, out_channels/latent_channels)\n",
        "        out = self.W_O(out) # project v_channels to out_channels/latent_channels\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, in_channels, nheads, dropout=0.0):\n",
        "        super().__init__()\n",
        "        assert in_channels % nheads == 0\n",
        "\n",
        "        self.W_Q = nn.Linear(in_channels, in_channels, bias=False)\n",
        "        self.W_K = nn.Linear(in_channels, in_channels, bias=False)\n",
        "        \n",
        "        self.W_V = nn.Linear(in_channels, in_channels, bias=False)\n",
        "        self.W_O = nn.Linear(in_channels, in_channels, bias=False)\n",
        "\n",
        "        self.head_dim = in_channels // nheads\n",
        "        self.nheads = nheads\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, in_channels = x.size()\n",
        "        \n",
        "        Q = self.W_Q(x).reshape(batch_size, seq_len, self.nheads, self.head_dim).transpose(1, 2)\n",
        "        K = self.W_K(x).reshape(batch_size, seq_len, self.nheads, self.head_dim).transpose(1, 2)\n",
        "        V = self.W_V(x).reshape(batch_size, seq_len, self.nheads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # (batch_size, nheads, seq_len, seq_len)\n",
        "        attn = (Q @ K.transpose(-2, -1)) / (1.0 * math.sqrt(self.head_dim))\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        \n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        # (batch_size, nheads, seq_len, head_dim)\n",
        "        out = attn @ V\n",
        "        # (batch_size, seq_len, in_channels)\n",
        "        out = out.transpose(1, 2).reshape(batch_size, seq_len, in_channels)\n",
        "\n",
        "        # (batch_size, seq_len, in_channels)\n",
        "        out = self.W_O(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_channels, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_channels, 4 * in_channels)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(4 * in_channels, in_channels)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, in_channels, nheads, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(in_channels)\n",
        "        self.attn = SelfAttention(in_channels, nheads, dropout)\n",
        "        self.ln_2 = nn.LayerNorm(in_channels)\n",
        "        self.mlp = MLP(in_channels, dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class CrossAttentionBlock(nn.Module):\n",
        "    def __init__(self, latent_channels, in_channels, nxheads, dropout=0.0):\n",
        "        ''' each cross-attention is followed by an MLP in the Perciever '''\n",
        "        super().__init__()\n",
        "        self.xattn = CrossAttention(\n",
        "            latent_channels,\n",
        "            in_channels,\n",
        "            nxheads=nxheads,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        self.mlp = MLP(latent_channels, dropout=dropout)\n",
        "        \n",
        "        self.ln_latent = nn.LayerNorm(latent_channels)\n",
        "        self.ln_input = nn.LayerNorm(in_channels)\n",
        "        self.ln_mlp = nn.LayerNorm(latent_channels)\n",
        "\n",
        "    def forward(self, latents, x):\n",
        "        latents = latents + self.xattn(self.ln_latent(latents), self.ln_input(x))\n",
        "        latents = latents + self.mlp(self.ln_mlp(latents))\n",
        "\n",
        "        return latents\n",
        "\n",
        "\n",
        "class PerceiverBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        latent_channels,\n",
        "        in_channels,\n",
        "        nheads,\n",
        "        nxheads,\n",
        "        nlayers,\n",
        "        dropout=0.0\n",
        "    ):\n",
        "        ''' PerceiverBlock is one CrossAttentionBlock followed by nlayer standard TransformerBlocks '''\n",
        "        super().__init__()\n",
        "        self.xattn_block = CrossAttentionBlock(latent_channels, in_channels, nxheads=nxheads, dropout=dropout)\n",
        "        self.attn_blocks = nn.ModuleList([\n",
        "            TransformerBlock(latent_channels, nheads=nheads, dropout=dropout)\n",
        "            for _ in range(nlayers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, latents, x):\n",
        "        latents = self.xattn_block(latents, x)\n",
        "        for block in self.attn_blocks:\n",
        "            latents = block(latents)\n",
        "\n",
        "        return latents\n",
        "\n",
        "\n",
        "class PerceiverBase(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        latent_channels,\n",
        "        latent_seq_len,\n",
        "        in_channels,\n",
        "        input_seq_len,\n",
        "        nheads,\n",
        "        nxheads,\n",
        "        nlayers,\n",
        "        nblocks,\n",
        "        pos_emb_channels,\n",
        "        dropout=0.0\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.pos_emb = nn.Embedding(input_seq_len, pos_emb_channels)\n",
        "        self.latents = LatentEmbeddings(latent_seq_len, latent_channels)\n",
        "        self.perceiver_blocks = nn.ModuleList([\n",
        "            PerceiverBlock(\n",
        "                latent_channels,\n",
        "                in_channels + pos_emb_channels,\n",
        "                nheads,\n",
        "                nxheads,\n",
        "                nlayers,\n",
        "                dropout\n",
        "            )\n",
        "            for _ in range(nblocks)\n",
        "        ])\n",
        "\n",
        "    def forward(self, input):\n",
        "        batch_size, seq_len, in_channels = input.size()\n",
        "        \n",
        "        pos = torch.arange(0, seq_len, dtype=torch.long, device=input.device)\n",
        "        pos_emb = self.pos_emb(pos).expand(batch_size, -1, -1)\n",
        "        \n",
        "        input = torch.cat([input, pos_emb], dim=-1)\n",
        "\n",
        "        latents = self.latents(batch_size)\n",
        "\n",
        "        for block in self.perceiver_blocks:\n",
        "            latents = block(latents, input)\n",
        "            \n",
        "        return latents\n",
        "\n",
        "\n",
        "class PerceiverClassificationHead(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        latent_channels,\n",
        "        latent_seq_len,\n",
        "        in_channels,\n",
        "        input_seq_len,\n",
        "        out_channels,\n",
        "        nheads,\n",
        "        nxheads,\n",
        "        nlayers,\n",
        "        nblocks,\n",
        "        pos_emb_channels,\n",
        "        dropout=0.0\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.perceiver = PerceiverBase(\n",
        "            latent_channels,\n",
        "            latent_seq_len,\n",
        "            in_channels,\n",
        "            input_seq_len,\n",
        "            nheads,\n",
        "            nxheads,\n",
        "            nlayers,\n",
        "            nblocks,\n",
        "            pos_emb_channels,\n",
        "            dropout\n",
        "        )\n",
        "        \n",
        "        self.head = nn.Linear(latent_channels, out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.perceiver(x)\n",
        "        x = torch.mean(x, dim=-2)\n",
        "        x = self.head(x) # logits for classification\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "lNFLDEz0phN9"
      },
      "outputs": [],
      "source": [
        "class CIFAR10DataModule(pl.LightningDataModule):\n",
        "    def __init__(self, batch_size=64):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "            transforms.Lambda(lambda x: x.view(3, -1).t())\n",
        "        ])\n",
        "\n",
        "    def prepare_data(self):\n",
        "        torchvision.datasets.CIFAR10(root='./data', train=True, download=True)\n",
        "        torchvision.datasets.CIFAR10(root='./data', train=False, download=True)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        if stage in ('fit', None):\n",
        "            self.cifar10_train = torchvision.datasets.CIFAR10(\n",
        "                root='./data', train=True, transform=self.transform)\n",
        "            self.cifar10_val = torchvision.datasets.CIFAR10(\n",
        "                root='./data', train=False, transform=self.transform)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.cifar10_train,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.cifar10_val,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "\n",
        "class CIFAR10Classifier(pl.LightningModule):\n",
        "    def __init__(self, model, learning_rate=3e-4):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        \n",
        "        self.loss = torch.nn.CrossEntropyLoss()\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        self.train_accuracy = Accuracy(task=\"multiclass\", num_classes=10)\n",
        "        self.val_accuracy = Accuracy(task=\"multiclass\", num_classes=10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        \n",
        "        loss = self.loss(logits, y)\n",
        "        acc = self.train_accuracy(logits, y)\n",
        "        \n",
        "        self.log('train/loss', loss, prog_bar=True)\n",
        "        self.log('train/accuracy', acc)\n",
        "        \n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        \n",
        "        loss = self.loss(logits, y)\n",
        "        acc = self.val_accuracy(logits, y)\n",
        "        \n",
        "        self.log('val/loss', loss, prog_bar=True)\n",
        "        self.log('val/accuracy', acc)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
        "        scheduler = OneCycleLR(optimizer, max_lr=self.learning_rate, total_steps=self.trainer.estimated_stepping_batches, anneal_strategy='linear')\n",
        "        print(f'total_steps: {self.trainer.estimated_stepping_batches}')\n",
        "        \n",
        "        return { \"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"train/loss\" }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "otaEgJVhm-wB"
      },
      "outputs": [],
      "source": [
        "input_seq_len = 1024 # M in paper, 32*32 for CIFAR-10\n",
        "in_channels = 3 # kv_dim in huggingface impl.\n",
        "pos_emb_channels = in_channels\n",
        "\n",
        "latent_seq_len = 128 # N in paper\n",
        "latent_channels = 256 # q_dim in huggingface impl.\n",
        "\n",
        "out_channels = 10 # CIFAR-10 has 10 classes\n",
        "\n",
        "nheads = 8\n",
        "nxheads = 1\n",
        "nlayers = 4\n",
        "nblocks = 2\n",
        "\n",
        "batch_size = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjefxbxvmYYJ"
      },
      "outputs": [],
      "source": [
        "model = PerceiverClassificationHead(\n",
        "    latent_channels,\n",
        "    latent_seq_len,\n",
        "    in_channels,\n",
        "    input_seq_len,\n",
        "    out_channels,\n",
        "    nheads,\n",
        "    nxheads,\n",
        "    nlayers,\n",
        "    nblocks,\n",
        "    pos_emb_channels,\n",
        "    dropout=0.1\n",
        ")\n",
        "\n",
        "cifar10_data = CIFAR10DataModule(batch_size=batch_size)\n",
        "cifar10_classifier = CIFAR10Classifier(model)\n",
        "\n",
        "wandb.finish()\n",
        "wandb_logger = WandbLogger(project=\"Perceiver CIFAR-10\", log_model=False)\n",
        "wandb_logger.watch(cifar10_classifier, log=\"all\")\n",
        "\n",
        "lr_monitor = pl.callbacks.LearningRateMonitor(logging_interval='step')\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=40,\n",
        "    devices=1,\n",
        "    accelerator=\"gpu\",\n",
        "    precision=\"16-mixed\",\n",
        "    logger=wandb_logger,\n",
        "    callbacks=[lr_monitor]\n",
        "    # overfit_batches=1,\n",
        "    # log_every_n_steps=1,\n",
        "    # limit_val_batches=0\n",
        ")\n",
        "\n",
        "tuner = Tuner(trainer)\n",
        "tuner.lr_find(cifar10_classifier, datamodule=cifar10_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1rjaTZLmalr"
      },
      "outputs": [],
      "source": [
        "trainer.fit(cifar10_classifier, cifar10_data)\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgU5zVuT58E9"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
